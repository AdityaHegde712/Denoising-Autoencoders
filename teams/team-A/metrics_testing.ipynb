{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bb044f",
   "metadata": {},
   "source": [
    "### Metrics Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c886b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def save_training_logs(log_dict, output_path=\"./outputs/training_log.json\"):\n",
    "    \"\"\"\n",
    "    Save the training history to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        log_dict: Dictionary containing training metrics for each epoch\n",
    "                  Format: {epoch_num: {'loss': ..., 'psnr': ..., 'val_loss': ..., 'val_psnr': ...}}\n",
    "        output_path: Where to save the file\n",
    "    \n",
    "    Example of what gets saved:\n",
    "    {\n",
    "        \"1\": {\"loss\": 0.123, \"psnr\": 25.4, \"val_loss\": 0.145, \"val_psnr\": 24.1},\n",
    "        \"2\": {\"loss\": 0.098, \"psnr\": 27.2, \"val_loss\": 0.112, \"val_psnr\": 26.5},\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Make sure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save as JSON (human-readable format)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(log_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"Training logs saved to: {output_path}\")\n",
    "    \n",
    "    # Also save as CSV for easy viewing in Excel\n",
    "    csv_path = output_path.replace('.json', '.csv')\n",
    "    \n",
    "    # Convert the dictionary to a pandas DataFrame\n",
    "    # Each row is one epoch, columns are the metrics\n",
    "    df = pd.DataFrame.from_dict(log_dict, orient='index')\n",
    "    df.index.name = 'epoch'\n",
    "    df.to_csv(csv_path)\n",
    "    \n",
    "    print(f\"Training logs also saved as CSV: {csv_path}\")\n",
    "    \n",
    "    return df  # Return the dataframe in case we want to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d981d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(log_dict, save_path=\"./outputs/training_curves.png\"):\n",
    "    \"\"\"\n",
    "    Create graphs showing how the model improved during training.\n",
    "    \n",
    "    This creates two graphs:\n",
    "    1. Loss over time (training and validation)\n",
    "    2. PSNR over time (training and validation)\n",
    "    \n",
    "    Args:\n",
    "        log_dict: Dictionary with training history\n",
    "        save_path: Where to save the graph image\n",
    "    \"\"\"\n",
    "    # Convert dictionary to pandas DataFrame for easier plotting\n",
    "    df = pd.DataFrame.from_dict(log_dict, orient='index')\n",
    "    df.index = df.index.astype(int)  # Make sure epoch numbers are integers\n",
    "    df = df.sort_index()  # Sort by epoch number\n",
    "    \n",
    "    # Check if validation metrics exist\n",
    "    has_val_metrics = 'val_loss' in df.columns and 'val_psnr' in df.columns\n",
    "    \n",
    "    # Create a figure with 2 subplots side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Loss over epochs\n",
    "    axes[0].plot(df.index, df['loss'], label='Training Loss', marker='o', linewidth=2)\n",
    "    if has_val_metrics:\n",
    "        axes[0].plot(df.index, df['val_loss'], label='Validation Loss', marker='s', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('Loss over Training', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: PSNR over epochs\n",
    "    axes[1].plot(df.index, df['psnr'], label='Training PSNR', marker='o', linewidth=2)\n",
    "    if has_val_metrics:\n",
    "        axes[1].plot(df.index, df['val_psnr'], label='Validation PSNR', marker='s', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('PSNR (dB)', fontsize=12)\n",
    "    axes[1].set_title('PSNR over Training', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Training curves saved to: {save_path}\")\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n=== Training Summary ===\")\n",
    "    print(f\"Best Training Loss: {df['loss'].min():.6f} (Epoch {df['loss'].idxmin()})\")\n",
    "    if has_val_metrics:\n",
    "        print(f\"Best Validation Loss: {df['val_loss'].min():.6f} (Epoch {df['val_loss'].idxmin()})\")\n",
    "    print(f\"Best Training PSNR: {df['psnr'].max():.2f} dB (Epoch {df['psnr'].idxmax()})\")\n",
    "    if has_val_metrics:\n",
    "        print(f\"Best Validation PSNR: {df['val_psnr'].max():.2f} dB (Epoch {df['val_psnr'].idxmax()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7e3d07",
   "metadata": {},
   "source": [
    "### Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device, checkpoint_path=\"./outputs/best.pt\"):\n",
    "    \"\"\"\n",
    "    Test the trained model on the test dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The autoencoder model\n",
    "        test_loader: DataLoader for test images\n",
    "        device: 'cuda' or 'cpu'\n",
    "        checkpoint_path: Path to the saved best model\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with test metrics and some example images\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING THE MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load the best model weights\n",
    "    print(f\"\\nLoading best model from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    print(f\"Model loaded from epoch {checkpoint['epoch']}\")\n",
    "    \n",
    "    # Put model in evaluation mode (important!)\n",
    "    # This turns off things like dropout that are only used during training\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # We'll calculate metrics and store some examples\n",
    "    test_loss_meter = AverageMeter()\n",
    "    test_psnr_meter = AverageMeter()\n",
    "    \n",
    "    # Store some example images to visualize later\n",
    "    example_noisy = []\n",
    "    example_denoised = []\n",
    "    example_clean = []\n",
    "    \n",
    "    print(\"\\nRunning model on test images...\")\n",
    "    \n",
    "    # We use torch.no_grad() because we're not training, just testing\n",
    "    # This saves memory and makes things faster\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (noisy, clean) in enumerate(test_loader):\n",
    "            # Move images to the device (GPU or CPU)\n",
    "            noisy = noisy.to(device)\n",
    "            clean = clean.to(device)\n",
    "            \n",
    "            # Pass noisy images through the model to get denoised version\n",
    "            denoised = model(noisy)\n",
    "            \n",
    "            # Calculate loss (how different is denoised from clean?)\n",
    "            loss = F.mse_loss(denoised, clean)\n",
    "            \n",
    "            # Calculate PSNR for each image in the batch\n",
    "            batch_mse = F.mse_loss(denoised, clean, reduction='none').view(clean.size(0), -1).mean(dim=1)\n",
    "            batch_psnr = psnr_from_mse(batch_mse).mean().item()\n",
    "            \n",
    "            # Update our running averages\n",
    "            test_loss_meter.update(loss.item(), n=clean.size(0))\n",
    "            test_psnr_meter.update(batch_psnr, n=clean.size(0))\n",
    "            \n",
    "            # Save first 8 images as examples\n",
    "            if batch_idx == 0:\n",
    "                num_examples = min(8, noisy.size(0))\n",
    "                example_noisy = noisy[:num_examples].cpu()\n",
    "                example_denoised = denoised[:num_examples].cpu()\n",
    "                example_clean = clean[:num_examples].cpu()\n",
    "    \n",
    "    # Print final test results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Test Loss (MSE): {test_loss_meter.avg:.6f}\")\n",
    "    print(f\"Test PSNR: {test_psnr_meter.avg:.2f} dB\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Return everything\n",
    "    return {\n",
    "        'test_loss': test_loss_meter.avg,\n",
    "        'test_psnr': test_psnr_meter.avg,\n",
    "        'example_noisy': example_noisy,\n",
    "        'example_denoised': example_denoised,\n",
    "        'example_clean': example_clean\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(example_noisy, example_denoised, example_clean, save_path=\"./outputs/test_results.png\"):\n",
    "    \"\"\"\n",
    "    Create a visual comparison of noisy, denoised, and clean images.\n",
    "    \n",
    "    This creates a grid showing:\n",
    "    - Row 1: Noisy input images\n",
    "    - Row 2: Model's denoised output\n",
    "    - Row 3: Ground truth (actual clean images)\n",
    "    \n",
    "    Args:\n",
    "        example_noisy: Tensor of noisy images\n",
    "        example_denoised: Tensor of denoised images from model\n",
    "        example_clean: Tensor of ground truth clean images\n",
    "        save_path: Where to save the comparison image\n",
    "    \"\"\"\n",
    "    \n",
    "    def denormalize(img):\n",
    "        \"\"\"Convert images from [-1, 1] back to [0, 1] for display\"\"\"\n",
    "        return (img + 1) / 2\n",
    "    \n",
    "    num_examples = example_noisy.size(0)\n",
    "    \n",
    "    # Create figure with 3 rows (noisy, denoised, clean) and multiple columns\n",
    "    fig, axes = plt.subplots(3, num_examples, figsize=(num_examples * 2, 6))\n",
    "    \n",
    "    # If we only have one example, axes won't be 2D, so fix that\n",
    "    if num_examples == 1:\n",
    "        axes = axes.reshape(3, 1)\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        # Denormalize images (convert from [-1,1] to [0,1])\n",
    "        noisy_img = denormalize(example_noisy[i]).permute(1, 2, 0).numpy()\n",
    "        denoised_img = denormalize(example_denoised[i]).permute(1, 2, 0).numpy()\n",
    "        clean_img = denormalize(example_clean[i]).permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Clip to valid range [0, 1]\n",
    "        noisy_img = np.clip(noisy_img, 0, 1)\n",
    "        denoised_img = np.clip(denoised_img, 0, 1)\n",
    "        clean_img = np.clip(clean_img, 0, 1)\n",
    "        \n",
    "        # Row 1: Noisy images\n",
    "        axes[0, i].imshow(noisy_img)\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('Noisy Input', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Row 2: Denoised images\n",
    "        axes[1, i].imshow(denoised_img)\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('Model Output\\n(Denoised)', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Row 3: Clean ground truth\n",
    "        axes[2, i].imshow(clean_img)\n",
    "        axes[2, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[2, i].set_title('Ground Truth\\n(Clean)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nTest result visualization saved to: {save_path}\")\n",
    "    \n",
    "    # Display in notebook\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_metrics(test_results, output_path=\"./outputs/test_metrics.json\"):\n",
    "    \"\"\"\n",
    "    Save the final test metrics to a file.\n",
    "    \n",
    "    Args:\n",
    "        test_results: Dictionary with test_loss and test_psnr\n",
    "        output_path: Where to save the metrics\n",
    "    \"\"\"\n",
    "    # Remove the image tensors from what we save (they're too big)\n",
    "    metrics = {\n",
    "        'test_loss': test_results['test_loss'],\n",
    "        'test_psnr': test_results['test_psnr']\n",
    "    }\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"Test metrics saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504abc6",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: After training is complete, you'll have log_dict from the fit() function\n",
    "# The fit() function returns: best_loss, log_dict = fit(...)\n",
    "# Let's assume log_dict is already available from the training cell above\n",
    "\n",
    "# STEP 2: Save and visualize training logs\n",
    "print(\"Saving training logs...\")\n",
    "training_df = save_training_logs(log_dict)\n",
    "\n",
    "print(\"\\nCreating training visualizations...\")\n",
    "plot_training_history(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Test the model on the test dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING PHASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_results = test_model(\n",
    "    model=model,\n",
    "    test_loader=test_NIDS_loader,\n",
    "    device=device,\n",
    "    checkpoint_path=\"./outputs/best.pt\"  # This is where fit() saved the best model\n",
    ")\n",
    "\n",
    "# STEP 4: Save test metrics\n",
    "save_test_metrics(test_results)\n",
    "\n",
    "# STEP 5: Visualize the results\n",
    "print(\"\\nCreating result visualizations...\")\n",
    "visualize_results(\n",
    "    test_results['example_noisy'],\n",
    "    test_results['example_denoised'],\n",
    "    test_results['example_clean']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL DONE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCheck the ./outputs folder for:\")\n",
    "print(\"  - training_log.json & .csv (training history)\")\n",
    "print(\"  - training_curves.png (loss and PSNR graphs)\")\n",
    "print(\"  - test_metrics.json (final test performance)\")\n",
    "print(\"  - test_results.png (visual comparison of images)\")\n",
    "print(\"  - best.pt (best model checkpoint)\")\n",
    "print(\"  - last.pt (last epoch checkpoint)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
